{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align = 'center'><font color = '#28abe3'>Red Hat Data Pre-processing</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have some ideas about how to treat the data, potential future predictors etc I need to convert it into a usable form, meaning I need to convert all the categorical data to something scikitlearn can handle. My first instinct would be to create a transformation of the categories to the natural numbers $$C \\rightarrow \\mathbb{N}$$\n",
    "such that the result is monotonically increasing w.r.t. its' emperical probability of a positive i.e. $$\\forall \\: i \\: s.t \\: C_i \\in C$$ we define a function f that approximates the emperical probability $$P(Outcome=1|X=C_i) \\approx f(C_i) =  \\frac{len(\\{Outcomes = 1 \\land  X = C_i\\})}{len(\\{X = C_i\\})}$$ and we create an ordered list $$Z = \\{f(C_a),f(C_b),....f(C_l)\\},\\:f(C_a) < f(C_b) < ... < f(C_l)$$ and we define our transform $$C_i \\rightarrow k$$ where k is the index of Z such that $$Z_k = f(C_i)$$.\n",
    "\n",
    "----\n",
    "\n",
    "The problem with this however is that we are given a dataset of approx 2 million datapoints. This is bordering on computationally infeasible. A much easier computation would be to map each $$C_i \\rightarrow i$$ This may generate relationships that are harder to teach but at least the computation is doable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rename columns\n",
    "act_train = pd.read_csv('act_train.csv')\n",
    "peopleData = pd.read_csv('people.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lets just deal with the people data first.\n",
    "for i in list(peopleData.columns):\n",
    "    if i not in ['people_id','activity_id','date']:\n",
    "        if peopleData[i].dtype == 'object':\n",
    "            peopleData[i] = peopleData[i].fillna('type 0')\n",
    "            peopleData[i] = peopleData[i].map(lambda x: x.split(' ')[1]).astype(np.int32)\n",
    "        elif peopleData[i].dtype == 'bool' :\n",
    "            peopleData[i] = peopleData[i].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now lets do it for the act_train dataset\n",
    "for i in list(act_train.columns):\n",
    "    if i not in ['outcome','people_id','activity_id','date']:\n",
    "        act_train[i] = act_train[i].fillna('type 0')\n",
    "        act_train[i] = act_train[i].map(lambda x: x.split(' ')[1]).astype(np.int32)\n",
    "    elif act_train[i].dtype == 'bool' :\n",
    "        act_train[i] = act_train[i].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMINDER: ALL NA VALUES ARE FILLED IN WITH ZERO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>people_id</th>\n",
       "      <th>activity_id</th>\n",
       "      <th>date_x</th>\n",
       "      <th>activity_category</th>\n",
       "      <th>char_1_x</th>\n",
       "      <th>char_2_x</th>\n",
       "      <th>char_3_x</th>\n",
       "      <th>char_4_x</th>\n",
       "      <th>char_5_x</th>\n",
       "      <th>char_6_x</th>\n",
       "      <th>...</th>\n",
       "      <th>char_29</th>\n",
       "      <th>char_30</th>\n",
       "      <th>char_31</th>\n",
       "      <th>char_32</th>\n",
       "      <th>char_33</th>\n",
       "      <th>char_34</th>\n",
       "      <th>char_35</th>\n",
       "      <th>char_36</th>\n",
       "      <th>char_37</th>\n",
       "      <th>char_38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_1734928</td>\n",
       "      <td>2023-08-26</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_2434093</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ppl_100</td>\n",
       "      <td>act2_3404049</td>\n",
       "      <td>2022-09-27</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  people_id   activity_id      date_x  activity_category  char_1_x  char_2_x  \\\n",
       "0   ppl_100  act2_1734928  2023-08-26                  4         0         0   \n",
       "1   ppl_100  act2_2434093  2022-09-27                  2         0         0   \n",
       "2   ppl_100  act2_3404049  2022-09-27                  2         0         0   \n",
       "\n",
       "   char_3_x  char_4_x  char_5_x  char_6_x   ...     char_29  char_30  char_31  \\\n",
       "0         0         0         0         0   ...           0        1        1   \n",
       "1         0         0         0         0   ...           0        1        1   \n",
       "2         0         0         0         0   ...           0        1        1   \n",
       "\n",
       "   char_32  char_33  char_34  char_35  char_36 char_37  char_38  \n",
       "0        0        0        1        1        1       0       36  \n",
       "1        0        0        1        1        1       0       36  \n",
       "2        0        0        1        1        1       0       36  \n",
       "\n",
       "[3 rows x 55 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join the dataframes\n",
    "trainData = act_train.merge(peopleData,on = 'people_id')\n",
    "trainData[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uyh that was trickier than I thought, okay, well at least we have a dataset that scikitlearn can interact with now. I think I should start with some simple benchmarks and go from there. Maybe a benchmark random forest then isolate the important predictors, work on some feature engineering and roll from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['people_id', 'activity_id', 'date_x', 'activity_category', 'char_1_x',\n",
       "       'char_2_x', 'char_3_x', 'char_4_x', 'char_5_x', 'char_6_x', 'char_7_x',\n",
       "       'char_8_x', 'char_9_x', 'char_10_x', 'outcome', 'char_1_y', 'group_1',\n",
       "       'char_2_y', 'date_y', 'char_3_y', 'char_4_y', 'char_5_y', 'char_6_y',\n",
       "       'char_7_y', 'char_8_y', 'char_9_y', 'char_10_y', 'char_11', 'char_12',\n",
       "       'char_13', 'char_14', 'char_15', 'char_16', 'char_17', 'char_18',\n",
       "       'char_19', 'char_20', 'char_21', 'char_22', 'char_23', 'char_24',\n",
       "       'char_25', 'char_26', 'char_27', 'char_28', 'char_29', 'char_30',\n",
       "       'char_31', 'char_32', 'char_33', 'char_34', 'char_35', 'char_36',\n",
       "       'char_37', 'char_38'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = trainData.drop('people_id',1)\n",
    "X = X.drop('activity_id',1)\n",
    "X = X.drop('date_x',1)\n",
    "X = X.drop('date_y',1)\n",
    "X = X.drop('outcome',1)\n",
    "Y = trainData['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split training and testing\n",
    "sample = np.random.permutation(len(X))\n",
    "X = X.iloc[sample]\n",
    "Y = Y.iloc[sample]\n",
    "xTrain = X[:1000000]\n",
    "yTrain = Y[:1000000]\n",
    "yTest = Y[1000000:1001000]\n",
    "xTest = X[1000000:1001000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=100, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forestBenchmark = RandomForestClassifier(n_estimators = 100,random_state=100)\n",
    "forestBenchmark.fit(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = forestBenchmark.predict(xTest)\n",
    "sum(predictions == yTest)/len(predictions)\n",
    "cma = confusion_matrix(yTest,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TPR = cma[0][0]/(cma[0][0]+cma[0][1])\n",
    "FPR = cma[1][0]/(cma[1][0]+cma[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ~ 0.942446570622\n"
     ]
    }
   ],
   "source": [
    "print('AUC ~', TPR*(1-FPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One point of confusion: With 1milli points of training data we get a score of .94 but only around .78 with 1000 datapoints. Additionally, it takes awhile for the forest to train on 1 mil datapoints so I dont want to work with the 1mil model. In order to do well at the end of the day i'll need to train on all the training data but im not sure if its' valid to assume model alterations that improve the AUC score w/ 1000 training points also improve the AUC score w/ 1mil training points an identical-ish amount.\n",
    "\n",
    "At least we have a good benchmark now and can focus on feature engineering.\n",
    "\n",
    "----\n",
    "Some Ideas: \n",
    "\n",
    "1.) Introduce the historical user data to make predictions.  (Strong)\n",
    "\n",
    "2.)Develop a better method for treating the NaN fields.  (Strong)\n",
    "\n",
    "3.) Consider transforms from categorical data that may be easier to learn.  (Weak)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
